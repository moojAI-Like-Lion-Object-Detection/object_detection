{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109879,"status":"ok","timestamp":1731382609567,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"7uZiocnitCu0","outputId":"ce32dad4-d5a3-44bb-c936-0a1c869a9ad4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6pds8SPn3Oez","executionInfo":{"status":"ok","timestamp":1731390422124,"user_tz":-540,"elapsed":3562,"user":{"displayName":"김형준","userId":"05139800887759179481"}},"outputId":"bc25bf6a-3529-4b9d-ee19-ea80e5bea76a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.2\n"]}]},{"cell_type":"markdown","source":["### 1. Data Split(Stratified Group K-Fold)"],"metadata":{"id":"AA9ZQap-5bNh"}},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731380238913,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"_Rypj7SZ1Ln7"},"outputs":[],"source":["# %%bash\n","\n","# mkdir /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset\n","# mkdir /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset/annotation\n","# cd /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset\n","# kaggle datasets download sshikamaru/udacity-self-driving-car-dataset\n","# chmod 777 udacity-self-driving-car-dataset.zip\n","# unzip udacity-self-driving-car-dataset.zip\n","# mv data/export/_annotations.csv /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset/annotation\n","# mv data/export .\n","# rm -rf data\n","# mv export data\n","# rm -rf udacity-self-driving-car-dataset.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731380238913,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"JyK7Cio9iF22"},"outputs":[],"source":["# import os\n","# import shutil\n","# import pandas as pd\n","# import numpy as np\n","# from sklearn.model_selection import StratifiedGroupKFold\n","# from tqdm import tqdm\n","# import yaml\n","\n","# # Base path for dataset\n","# base_path = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset/\"\n","# image_base_path = base_path + \"data/\"  # 이미지 파일들이 저장된 디렉토리\n","\n","# image_path_list = glob.glob(image_base_path+\"*\")\n","\n","# image_file_name = []\n","\n","# for image_path in image_path_list:\n","#   image_file_name.append(image_path.split('/')[9])\n","\n","# # Load the annotations CSV\n","# annotation = pd.read_csv(base_path + \"annotation/_annotations.csv\")\n","\n","# print(f\"previous annotation len: {len(annotation)}\")\n","\n","\n","# print(len(set(annotation['filename']) & set(image_file_name)))\n","\n","# intersection_filenames = set(annotation['filename']).intersection(image_file_name)\n","# print(len(intersection_filenames))\n","\n","\n","# # 'filename'이 합집합에 포함된 행만 선택\n","# annotation = annotation[annotation['filename'].isin(intersection_filenames)]\n","\n","\n","# # print(annotation.columns)  # Index(['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax'], dtype='object')\n","# # 1. 'class' 컬럼 이름을 'class_name'으로 변경\n","# annotation.rename(columns={'class': 'class_name'}, inplace=True)\n","\n","# # 2. 'class_name' 컬럼을 One-Hot Encoding하여 'class' 컬럼을 생성\n","# # One-Hot Encoding을 위한 컬럼 생성\n","# class_names = annotation['class_name'].unique()  # 고유한 클래스 이름을 추출\n","# class_mapping = {name: idx for idx, name in enumerate(class_names)}  # 클래스 이름과 인덱스 매핑\n","\n","# # 'class_name'을 'class' 컬럼으로 변환 (One-Hot Encoding)\n","# annotation['class'] = annotation['class_name'].map(class_mapping)\n","\n","\n","# # Create StratifiedGroupKFold\n","# train_sgkf = StratifiedGroupKFold(n_splits=3)\n","# train_fold = train_sgkf.split(annotation, annotation['class'], groups=annotation['filename'])\n","\n","# # Train, validation, and test splits\n","# train_fold = next(train_fold)\n","# train_annotation = annotation.iloc[train_fold[0]]\n","# tmp_annotation = annotation.iloc[train_fold[1]]\n","\n","# test_sgkf = StratifiedGroupKFold(n_splits=2)\n","# test_fold = test_sgkf.split(tmp_annotation, tmp_annotation['class'], tmp_annotation['filename'])\n","\n","# test_fold = next(test_fold)\n","# valid_annotation = tmp_annotation.iloc[test_fold[0]]\n","# test_annotation = tmp_annotation.iloc[test_fold[1]]\n","\n","\n","# train_annotation.drop_duplicates(['filename'], inplace = True)\n","# valid_annotation.drop_duplicates(['filename'], inplace = True)\n","# test_annotation.drop_duplicates(['filename'], inplace = True)\n","\n","# # Define directory structure for YOLO\n","# yolo_base_path = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/\"\n","# image_dirs = ['train', 'valid', 'test']\n","# label_dirs = ['train', 'valid', 'test']\n","\n","# # Create directories if they don't exist\n","# for image_dir, label_dir in zip(image_dirs, label_dirs):\n","#     os.makedirs(os.path.join(yolo_base_path, f\"images/{image_dir}\"), exist_ok=True)\n","#     os.makedirs(os.path.join(yolo_base_path, f\"labels/{label_dir}\"), exist_ok=True)\n","\n","\n","# # Function to convert annotations to YOLO format and move images\n","# def convert_to_yolo_format(annotation, image_set):\n","#     print(image_set)\n","#     # Paths for images and labels\n","#     image_dir = os.path.join(yolo_base_path, f\"images/{image_set}\")\n","#     label_dir = os.path.join(yolo_base_path, f\"labels/{image_set}\")\n","\n","#     # Loop over each annotation\n","#     for _, row in tqdm(annotation.iterrows()):\n","#         # Image filename and paths\n","#         image_filename = row['filename']\n","#         image_path = os.path.join(image_base_path, image_filename)  # 이미지는 .data 폴더에서 찾기\n","#         label_filename = os.path.splitext(image_filename)[0] + '.txt'\n","#         label_path = os.path.join(label_dir, label_filename)\n","\n","#         # Ensure the label directory exists before writing the file\n","#         os.makedirs(label_dir, exist_ok=True)  # Make sure the label directory exists\n","\n","#         # Calculate YOLO format coordinates\n","#         img_width = row['width']\n","#         img_height = row['height']\n","\n","#         # YOLO format: class_id center_x center_y width height (all values are relative to image size)\n","#         center_x = (row['xmin'] + row['xmax']) / 2 / img_width\n","#         center_y = (row['ymin'] + row['ymax']) / 2 / img_height\n","#         obj_width = (row['xmax'] - row['xmin']) / img_width\n","#         obj_height = (row['ymax'] - row['ymin']) / img_height\n","\n","#         # Write to label file\n","#         with open(label_path, 'a') as label_file:\n","#             label_file.write(f\"{row['class']} {center_x} {center_y} {obj_width} {obj_height}\\n\")\n","\n","#         # Copy image to appropriate directory\n","#         shutil.copy(image_path, os.path.join(image_dir, image_filename))\n","\n","\n","# # Convert annotations for train, validation, and test datasets\n","# convert_to_yolo_format(train_annotation, 'train')\n","# convert_to_yolo_format(valid_annotation, 'valid')\n","# convert_to_yolo_format(test_annotation, 'test')\n","\n","# print(\"YOLO dataset prepared.\")\n","\n","\n","# # 3. data.yaml 파일 생성\n","# data_yaml = {\n","#     'train': '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/train',\n","#     'val': '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/valid',\n","#     'test': '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/test',\n","#     'nc': len(class_names),  # 클래스 개수 (One-Hot Encoding된 클래스의 개수)\n","#     'names': class_names.tolist()  # 클래스 이름 리스트\n","# }\n","\n","# # data.yaml 경로\n","# yaml_path = '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/data.yaml'\n","\n","# # YAML 파일로 저장\n","# with open(yaml_path, 'w') as f:\n","#     yaml.dump(data_yaml, f)"]},{"cell_type":"markdown","source":["### 2. Model"],"metadata":{"id":"TbOiyE_m6V2h"}},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":11366,"status":"ok","timestamp":1731390462877,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"wis2VPcsAvGD"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import gc\n","import os\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from tqdm import tqdm\n","import numpy as np\n","import torchvision.ops.boxes as box_ops\n","from PIL import Image\n","# from torchmetrics.detection.mean_ap import MeanAveragePrecision (commented out due to missing module)\n","# Install torchmetrics if needed: !pip install torchmetrics\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","\n","# Clear cache once\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","class ConvBNAct(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, act=True):\n","        super(ConvBNAct, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.act = nn.SiLU() if act else nn.Identity()\n","\n","    def forward(self, x):\n","        return self.act(self.bn(self.conv(x)))\n","\n","class BottleNeck(nn.Module):\n","    def __init__(self, in_channels, out_channels, shortcut=True):\n","        super(BottleNeck, self).__init__()\n","        self.conv1 = ConvBNAct(in_channels, out_channels, 1, 1, 0)\n","        self.conv2 = ConvBNAct(out_channels, out_channels, 3, 1, 1)\n","        self.shortcut = shortcut and in_channels == out_channels\n","\n","    def forward(self, x):\n","        out = self.conv2(self.conv1(x))\n","        return x + out if self.shortcut else out\n","\n","class C3(nn.Module):\n","    def __init__(self, in_channels, out_channels, n=3):\n","        super(C3, self).__init__()\n","        self.conv1 = ConvBNAct(in_channels, out_channels // 2, 1, 1, 0)\n","        self.conv2 = ConvBNAct(in_channels, out_channels // 2, 1, 1, 0)\n","        self.conv3 = ConvBNAct(out_channels, out_channels, 1, 1, 0)\n","        self.bottlenecks = nn.Sequential(*[BottleNeck(out_channels // 2, out_channels // 2) for _ in range(n)])\n","\n","    def forward(self, x):\n","        y1 = self.bottlenecks(self.conv1(x))\n","        y2 = self.conv2(x)\n","        return self.conv3(torch.cat((y1, y2), dim=1))\n","\n","class SPPF(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(SPPF, self).__init__()\n","        self.conv1 = ConvBNAct(in_channels, out_channels, 1, 1, 0)\n","        self.conv2 = ConvBNAct(out_channels * 4, out_channels, 1, 1, 0)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        y1 = F.max_pool2d(x, 5, stride=1, padding=2)\n","        y2 = F.max_pool2d(x, 9, stride=1, padding=4)\n","        y3 = F.max_pool2d(x, 13, stride=1, padding=6)\n","        return self.conv2(torch.cat([x, y1, y2, y3], dim=1))"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731377122650,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"P24WdNkO-lhP"},"outputs":[],"source":["class YOLOv5(nn.Module):\n","    def initialize_anchors(self):\n","        # Anchors initialization for three scales (updated to include anchors)\n","        # Anchors for the three scales\n","        self.anchors = [\n","            [10, 13, 16, 30, 33, 23],  # Small scale anchors\n","            [30, 61, 62, 45, 59, 119],  # Medium scale anchors\n","            [116, 90, 156, 198, 373, 326]  # Large scale anchors\n","        ]\n","    def __init__(self, num_classes=11):\n","\n","        self.initialize_anchors()  # Initialize anchors for detection layers\n","        super(YOLOv5, self).__init__()\n","\n","        self.num_classes = num_classes\n","\n","        # Backbone\n","        self.conv1 = ConvBNAct(3, 64, 6, 2, 2)\n","        self.conv2 = ConvBNAct(64, 128, 3, 2, 1)\n","        self.c3_1 = C3(128, 128, 3)\n","        self.conv3 = ConvBNAct(128, 256, 3, 2, 1)\n","        self.c3_2 = C3(256, 256, 6)\n","        self.conv4 = ConvBNAct(256, 512, 3, 2, 1)\n","        self.c3_3 = C3(512, 512, 9)\n","        self.conv5 = ConvBNAct(512, 1024, 3, 2, 1)\n","        self.sppf = SPPF(1024, 1024)\n","\n","        # Neck\n","        self.c3_neck_1 = C3(1024 + 512, 512, 3)  # Adjusted to handle 1024+512=1536 channels\n","        self.c3_neck_2 = C3(512 + 256, 256, 3)\n","\n","        # Upsample layers\n","        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n","\n","        # Prediction Layers (Head)\n","        self.detect_large = nn.Conv2d(1024, 3 * (self.num_classes + 5), 1)  # Large scale detection\n","        self.detect_medium = nn.Conv2d(512, 3 * (self.num_classes + 5), 1)  # Medium scale detection\n","        self.detect_small = nn.Conv2d(256, 3 * (self.num_classes + 5), 1)  # Small scale detection\n","\n","    def forward(self, x):\n","        # Backbone\n","        x1 = self.conv1(x)\n","        x2 = self.conv2(x1)\n","        x2 = self.c3_1(x2)\n","        x3 = self.conv3(x2)\n","        x3 = self.c3_2(x3)\n","        x4 = self.conv4(x3)\n","        x4 = self.c3_3(x4)\n","        x5 = self.conv5(x4)\n","        x5 = self.sppf(x5)\n","\n","        # Large scale detection\n","        detect_large = self.detect_large(x5)  # Output size: [batch_size, (num_classes + 5), h, w]\n","\n","        # Neck\n","        x5_upsampled = self.upsample(x5)\n","        x4_concat = torch.cat((x5_upsampled, x4), dim=1)\n","        x4 = self.c3_neck_1(x4_concat)\n","\n","        # Medium scale detection\n","        detect_medium = self.detect_medium(x4)\n","\n","        x4_upsampled = self.upsample(x4)\n","        x3_concat = torch.cat((x4_upsampled, x3), dim=1)\n","        x3 = self.c3_neck_2(x3_concat)\n","\n","        # Small scale detection\n","        detect_small = self.detect_small(x3)\n","\n","        # 각 출력값을 반환할 때 바운드인포스, confidence, 클래스 분류\n","        predictions = [detect_small, detect_medium, detect_large]\n","\n","        # 바운드 박스, confidence, 클래스 정보 분류\n","        pred_boxes = [pred[..., :4] for pred in predictions]  # [x_center, y_center, w, h]\n","        pred_conf = [torch.sigmoid(pred[..., 4:5]) for pred in predictions]  # confidence (sigmoid 적용)\n","        pred_cls = [torch.sigmoid(pred[..., 5:]) for pred in predictions]    # 클래스 (sigmoid 적용)\n","\n","        return pred_boxes, pred_conf, pred_cls"]},{"cell_type":"markdown","source":["### 3. Custom Dataset, DataLoader"],"metadata":{"id":"HDj7yXZT6c74"}},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731377122650,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"g32GDcQsgYNf"},"outputs":[],"source":["# Dataset improvements and fixes\n","class ObjectDetectionDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transforms=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n","        self.image_files = self.image_files[:len(self.image_files)//10]\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        image_path = os.path.join(self.image_dir, self.image_files[idx])\n","        image = Image.open(image_path).convert(\"RGB\")\n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        # Load label\n","        label_path = os.path.join(self.label_dir, os.path.splitext(self.image_files[idx])[0] + '.txt')\n","        boxes = []\n","        labels = []\n","        with open(label_path, 'r') as f:\n","            for line in f.readlines():\n","                label, x_center, y_center, width, height = map(float, line.strip().split())\n","                labels.append(int(label))\n","                # YOLOv5 expects [x_center, y_center, width, height] normalized\n","                boxes.append([x_center, y_center, width, height])\n","\n","        boxes = torch.tensor(boxes, dtype=torch.float32)\n","        labels = torch.tensor(labels, dtype=torch.long)\n","\n","        # YOLOv5 expects targets in the format: [class, x_center, y_center, width, height]\n","        targets = torch.cat((labels.unsqueeze(1).float(), boxes), dim=1)\n","\n","        return image, targets\n","\n","# Image transformations including normalization\n","image_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","source":["def compute_loss(predictions, targets, anchors):\n","    # Computes the loss using anchor boxes for more accurate localization\n","    batch_size = targets.size(0)\n","    num_anchors = 3  # Assuming 3 anchors per detection layer\n","    targets = targets.view(batch_size, -1, 5)\n","    expanded_targets = targets.unsqueeze(1).expand(-1, num_anchors, -1, -1).contiguous().view(batch_size, -1, 5)\n","    pred_boxes, pred_conf, pred_cls = predictions\n","    anchor_tensors = [\n","        torch.tensor(anchor, device=targets.device).float().view(-1, 2) for anchor in anchors\n","    ]  # Create anchor tensors for each detection scale  # Create anchor tensors for each detection scale\n","    box_loss = sum(\n","        F.mse_loss(pred_box.view(batch_size, -1, 4), expanded_targets[..., 1:5]) for pred_box, anchor in zip(pred_boxes, anchor_tensors)\n","    )  # Calculate box loss using anchors\n","    conf_loss = sum(criterion(pred_conf[i].view(batch_size, -1, 1), expanded_targets[..., 0:1]) for i in range(len(pred_conf)))\n","    cls_loss = sum(criterion(pred_cls[i].view(batch_size, -1, num_classes), expanded_targets[..., 0:1].expand_as(pred_cls[i].view(batch_size, -1, num_classes))) for i in range(len(pred_cls)))\n","    return box_loss + conf_loss + cls_loss"],"metadata":{"id":"3hMAwZPAOwPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print summary\n","from torchsummary import summary\n","\n","\n","# Instantiate the model and print the summary with smaller input size to fit memory constraints\n","model = YOLOv5(num_classes=11).to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","summary(model, (3, 512, 512))"],"metadata":{"id":"VrzH7Ko9OwLf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4. Train"],"metadata":{"id":"ZxbB4IhJ6nYL"}},{"cell_type":"code","source":["# Hyperparameters\n","num_classes = 11\n","batch_size = 16\n","learning_rate = 0.001\n","epochs = 30\n","\n","# Optimizer and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.BCEWithLogitsLoss()\n","mean_ap_metric = MeanAveragePrecision()\n","\n","# DataLoader\n","train_loader = DataLoader(ObjectDetectionDataset(\"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/train\", \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/train\", transforms=image_transforms),\n","                          batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n","val_loader = DataLoader(ObjectDetectionDataset(\"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/valid\", \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/valid\", transforms=image_transforms),\n","                        batch_size=1, shuffle=False, num_workers=4, pin_memory=True)"],"metadata":{"id":"2K2o4DenOwIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        print(images[0].shape) # torch.Size([3, 512, 512])\n","        print(targets[0].shape) # torch.Size([1, 5]) # class, x_center, y_center, width, height\n","\n","        optimizer.zero_grad()\n","        predictions = model(images)\n","        print(len(predictions)) # 3\n","\n","\n","        print(len(predictions[0])) # 3\n","        print(predictions[0])\n","        print(len(predictions[1])) # 3\n","        print(len(predictions[2])) # 3\n","\n","        loss = compute_loss(predictions, targets, model.anchors)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader)}\")\n","\n","    # Validation and mAP calculation\n","    model.eval()\n","    mean_ap_metric.reset()\n","    with torch.no_grad():\n","        for images, targets in val_loader:\n","            images, targets = images.to(device), targets.to(device)\n","            pred_boxes, pred_conf, pred_cls = model(images)\n","            detections = []\n","            for i in range(len(pred_boxes)):\n","                for j in range(len(pred_boxes[i])):\n","                    detection = torch.cat((pred_boxes[i][j], pred_conf[i][j], pred_cls[i][j]), dim=-1)\n","                    detections.append(detection)\n","            mean_ap_metric.update(detections, targets)\n","\n","    mAP = mean_ap_metric.compute()\n","    print(f\"Epoch {epoch + 1}, Validation mAP: {mAP}\")"],"metadata":{"id":"5ldiluK6OwEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        print(images[0].shape) # torch.Size([3, 512, 512])\n","        print(targets[0].shape) # torch.Size([1, 5]) # class, x_center, y_center, width, height\n","\n","        optimizer.zero_grad()\n","        predictions = model(images)\n","        print(len(predictions)) # 3\n","\n","\n","        print(len(predictions[0])) # 3\n","        print(predictions[0])\n","        print(len(predictions[1])) # 3\n","        print(len(predictions[2])) # 3\n","\n","        loss = compute_loss(predictions, targets, model.anchors)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader)}\")\n","\n","    # Validation and mAP calculation\n","    model.eval()\n","    mean_ap_metric.reset()\n","    with torch.no_grad():\n","        for images, targets in val_loader:\n","            images, targets = images.to(device), targets.to(device)\n","            pred_boxes, pred_conf, pred_cls = model(images)\n","            detections = []\n","            for i in range(len(pred_boxes)):\n","                for j in range(len(pred_boxes[i])):\n","                    detection = torch.cat((pred_boxes[i][j], pred_conf[i][j], pred_cls[i][j]), dim=-1)\n","                    detections.append(detection)\n","            mean_ap_metric.update(detections, targets)\n","\n","    mAP = mean_ap_metric.compute()\n","    print(f\"Epoch {epoch + 1}, Validation mAP: {mAP}\")"],"metadata":{"id":"PNEm7BX4OwBG"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMjM3tbTr924pVi/cBpmTDW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}