{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109879,"status":"ok","timestamp":1731382609567,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"7uZiocnitCu0","outputId":"ce32dad4-d5a3-44bb-c936-0a1c869a9ad4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install torchmetrics"],"metadata":{"id":"6pds8SPn3Oez"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731380238913,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"_Rypj7SZ1Ln7"},"outputs":[],"source":["# %%bash\n","\n","# mkdir /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset\n","# mkdir /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset/annotation\n","# cd /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset\n","# kaggle datasets download sshikamaru/udacity-self-driving-car-dataset\n","# chmod 777 udacity-self-driving-car-dataset.zip\n","# unzip udacity-self-driving-car-dataset.zip\n","# mv data/export/_annotations.csv /content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset/annotation\n","# mv data/export .\n","# rm -rf data\n","# mv export data\n","# rm -rf udacity-self-driving-car-dataset.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1731380238913,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"JyK7Cio9iF22"},"outputs":[],"source":["# import os\n","# import shutil\n","# import pandas as pd\n","# import numpy as np\n","# from sklearn.model_selection import StratifiedGroupKFold\n","# from tqdm import tqdm\n","# import yaml\n","\n","# # Base path for dataset\n","# base_path = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/dataset/\"\n","# image_base_path = base_path + \"data/\"  # 이미지 파일들이 저장된 디렉토리\n","\n","# image_path_list = glob.glob(image_base_path+\"*\")\n","\n","# image_file_name = []\n","\n","# for image_path in image_path_list:\n","#   image_file_name.append(image_path.split('/')[9])\n","\n","# # Load the annotations CSV\n","# annotation = pd.read_csv(base_path + \"annotation/_annotations.csv\")\n","\n","# print(f\"previous annotation len: {len(annotation)}\")\n","\n","\n","# print(len(set(annotation['filename']) & set(image_file_name)))\n","\n","# intersection_filenames = set(annotation['filename']).intersection(image_file_name)\n","# print(len(intersection_filenames))\n","\n","\n","# # 'filename'이 합집합에 포함된 행만 선택\n","# annotation = annotation[annotation['filename'].isin(intersection_filenames)]\n","\n","\n","# # print(annotation.columns)  # Index(['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax'], dtype='object')\n","# # 1. 'class' 컬럼 이름을 'class_name'으로 변경\n","# annotation.rename(columns={'class': 'class_name'}, inplace=True)\n","\n","# # 2. 'class_name' 컬럼을 One-Hot Encoding하여 'class' 컬럼을 생성\n","# # One-Hot Encoding을 위한 컬럼 생성\n","# class_names = annotation['class_name'].unique()  # 고유한 클래스 이름을 추출\n","# class_mapping = {name: idx for idx, name in enumerate(class_names)}  # 클래스 이름과 인덱스 매핑\n","\n","# # 'class_name'을 'class' 컬럼으로 변환 (One-Hot Encoding)\n","# annotation['class'] = annotation['class_name'].map(class_mapping)\n","\n","\n","# # Create StratifiedGroupKFold\n","# train_sgkf = StratifiedGroupKFold(n_splits=3)\n","# train_fold = train_sgkf.split(annotation, annotation['class'], groups=annotation['filename'])\n","\n","# # Train, validation, and test splits\n","# train_fold = next(train_fold)\n","# train_annotation = annotation.iloc[train_fold[0]]\n","# tmp_annotation = annotation.iloc[train_fold[1]]\n","\n","# test_sgkf = StratifiedGroupKFold(n_splits=2)\n","# test_fold = test_sgkf.split(tmp_annotation, tmp_annotation['class'], tmp_annotation['filename'])\n","\n","# test_fold = next(test_fold)\n","# valid_annotation = tmp_annotation.iloc[test_fold[0]]\n","# test_annotation = tmp_annotation.iloc[test_fold[1]]\n","\n","\n","# train_annotation.drop_duplicates(['filename'], inplace = True)\n","# valid_annotation.drop_duplicates(['filename'], inplace = True)\n","# test_annotation.drop_duplicates(['filename'], inplace = True)\n","\n","# # Define directory structure for YOLO\n","# yolo_base_path = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/\"\n","# image_dirs = ['train', 'valid', 'test']\n","# label_dirs = ['train', 'valid', 'test']\n","\n","# # Create directories if they don't exist\n","# for image_dir, label_dir in zip(image_dirs, label_dirs):\n","#     os.makedirs(os.path.join(yolo_base_path, f\"images/{image_dir}\"), exist_ok=True)\n","#     os.makedirs(os.path.join(yolo_base_path, f\"labels/{label_dir}\"), exist_ok=True)\n","\n","\n","# # Function to convert annotations to YOLO format and move images\n","# def convert_to_yolo_format(annotation, image_set):\n","#     print(image_set)\n","#     # Paths for images and labels\n","#     image_dir = os.path.join(yolo_base_path, f\"images/{image_set}\")\n","#     label_dir = os.path.join(yolo_base_path, f\"labels/{image_set}\")\n","\n","#     # Loop over each annotation\n","#     for _, row in tqdm(annotation.iterrows()):\n","#         # Image filename and paths\n","#         image_filename = row['filename']\n","#         image_path = os.path.join(image_base_path, image_filename)  # 이미지는 .data 폴더에서 찾기\n","#         label_filename = os.path.splitext(image_filename)[0] + '.txt'\n","#         label_path = os.path.join(label_dir, label_filename)\n","\n","#         # Ensure the label directory exists before writing the file\n","#         os.makedirs(label_dir, exist_ok=True)  # Make sure the label directory exists\n","\n","#         # Calculate YOLO format coordinates\n","#         img_width = row['width']\n","#         img_height = row['height']\n","\n","#         # YOLO format: class_id center_x center_y width height (all values are relative to image size)\n","#         center_x = (row['xmin'] + row['xmax']) / 2 / img_width\n","#         center_y = (row['ymin'] + row['ymax']) / 2 / img_height\n","#         obj_width = (row['xmax'] - row['xmin']) / img_width\n","#         obj_height = (row['ymax'] - row['ymin']) / img_height\n","\n","#         # Write to label file\n","#         with open(label_path, 'a') as label_file:\n","#             label_file.write(f\"{row['class']} {center_x} {center_y} {obj_width} {obj_height}\\n\")\n","\n","#         # Copy image to appropriate directory\n","#         shutil.copy(image_path, os.path.join(image_dir, image_filename))\n","\n","\n","# # Convert annotations for train, validation, and test datasets\n","# convert_to_yolo_format(train_annotation, 'train')\n","# convert_to_yolo_format(valid_annotation, 'valid')\n","# convert_to_yolo_format(test_annotation, 'test')\n","\n","# print(\"YOLO dataset prepared.\")\n","\n","\n","# # 3. data.yaml 파일 생성\n","# data_yaml = {\n","#     'train': '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/train',\n","#     'val': '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/valid',\n","#     'test': '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/test',\n","#     'nc': len(class_names),  # 클래스 개수 (One-Hot Encoding된 클래스의 개수)\n","#     'names': class_names.tolist()  # 클래스 이름 리스트\n","# }\n","\n","# # data.yaml 경로\n","# yaml_path = '/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/data.yaml'\n","\n","# # YAML 파일로 저장\n","# with open(yaml_path, 'w') as f:\n","#     yaml.dump(data_yaml, f)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1731380238913,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"dLiYQvwy7R6K"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3887,"status":"ok","timestamp":1731382613450,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"xpLbLQJMAtiC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"663f5715-d4d4-4bb4-ea76-3e3fbe7d3f10"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchmetrics\n","  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.0+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.11.8 torchmetrics-1.5.2\n"]}],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1731380286420,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"riOkBj-5FcLN"},"outputs":[],"source":["# 여기여기"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3072,"status":"ok","timestamp":1731382616520,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"AK6VROzrFhMU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f28fe50-6367-4325-f862-39fb65fbf311"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["30"]},"metadata":{},"execution_count":3}],"source":["import torch\n","import gc\n","\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"cell_type":"code","source":[],"metadata":{"id":"ZpzxgM37lg3O","executionInfo":{"status":"ok","timestamp":1731385218388,"user_tz":-540,"elapsed":2,"user":{"displayName":"김형준","userId":"05139800887759179481"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1DcJ771gl4CH","executionInfo":{"status":"ok","timestamp":1731385217179,"user_tz":-540,"elapsed":1041,"user":{"displayName":"김형준","userId":"05139800887759179481"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":2664,"status":"error","timestamp":1731386980685,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"wis2VPcsAvGD","colab":{"base_uri":"https://localhost:8080/","height":558},"outputId":"2975caf2-732d-4b35-ccdb-2bfb62a6deab"},"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch 1/30:   0%|          | 0/1753 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 512, 512])\n","torch.Size([1, 5])\n","3\n","3\n","3\n","3\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-30-3548321c395c>:223: UserWarning: Using a target size (torch.Size([1, 3, 4])) that is different to the input size (torch.Size([1, 3072, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  F.mse_loss(pred_box.view(batch_size, -1, 4), expanded_targets[..., 1:5]) for pred_box, anchor in zip(pred_boxes, anchor_tensors)\n","Epoch 1/30:   0%|          | 0/1753 [00:01<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (3072) must match the size of tensor b (3) at non-singleton dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-3548321c395c>\u001b[0m in \u001b[0;36m<cell line: 230>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-3548321c395c>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(predictions, targets, anchors)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     ]  # Create anchor tensors for each detection scale  # Create anchor tensors for each detection scale\n\u001b[0;32m--> 222\u001b[0;31m     box_loss = sum(\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     )  # Calculate box loss using anchors\n","\u001b[0;32m<ipython-input-30-3548321c395c>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    221\u001b[0m     ]  # Create anchor tensors for each detection scale  # Create anchor tensors for each detection scale\n\u001b[1;32m    222\u001b[0m     box_loss = sum(\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_box\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred_box\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     )  # Calculate box loss using anchors\n\u001b[1;32m    225\u001b[0m     \u001b[0mconf_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_conf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_conf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3789\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3791\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3792\u001b[0m     return torch._C._nn.mse_loss(\n\u001b[1;32m   3793\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3072) must match the size of tensor b (3) at non-singleton dimension 1"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import gc\n","import os\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from tqdm import tqdm\n","import numpy as np\n","import torchvision.ops.boxes as box_ops\n","from PIL import Image\n","# from torchmetrics.detection.mean_ap import MeanAveragePrecision (commented out due to missing module)\n","# Install torchmetrics if needed: !pip install torchmetrics\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","\n","# Clear cache once\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","class ConvBNAct(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, act=True):\n","        super(ConvBNAct, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n","        self.bn = nn.BatchNorm2d(out_channels)\n","        self.act = nn.SiLU() if act else nn.Identity()\n","\n","    def forward(self, x):\n","        return self.act(self.bn(self.conv(x)))\n","\n","class BottleNeck(nn.Module):\n","    def __init__(self, in_channels, out_channels, shortcut=True):\n","        super(BottleNeck, self).__init__()\n","        self.conv1 = ConvBNAct(in_channels, out_channels, 1, 1, 0)\n","        self.conv2 = ConvBNAct(out_channels, out_channels, 3, 1, 1)\n","        self.shortcut = shortcut and in_channels == out_channels\n","\n","    def forward(self, x):\n","        out = self.conv2(self.conv1(x))\n","        return x + out if self.shortcut else out\n","\n","class C3(nn.Module):\n","    def __init__(self, in_channels, out_channels, n=3):\n","        super(C3, self).__init__()\n","        self.conv1 = ConvBNAct(in_channels, out_channels // 2, 1, 1, 0)\n","        self.conv2 = ConvBNAct(in_channels, out_channels // 2, 1, 1, 0)\n","        self.conv3 = ConvBNAct(out_channels, out_channels, 1, 1, 0)\n","        self.bottlenecks = nn.Sequential(*[BottleNeck(out_channels // 2, out_channels // 2) for _ in range(n)])\n","\n","    def forward(self, x):\n","        y1 = self.bottlenecks(self.conv1(x))\n","        y2 = self.conv2(x)\n","        return self.conv3(torch.cat((y1, y2), dim=1))\n","\n","class SPPF(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(SPPF, self).__init__()\n","        self.conv1 = ConvBNAct(in_channels, out_channels, 1, 1, 0)\n","        self.conv2 = ConvBNAct(out_channels * 4, out_channels, 1, 1, 0)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        y1 = F.max_pool2d(x, 5, stride=1, padding=2)\n","        y2 = F.max_pool2d(x, 9, stride=1, padding=4)\n","        y3 = F.max_pool2d(x, 13, stride=1, padding=6)\n","        return self.conv2(torch.cat([x, y1, y2, y3], dim=1))\n","\n","class YOLOv5(nn.Module):\n","    def initialize_anchors(self):\n","        # Anchors initialization for three scales (updated to include anchors)\n","        # Anchors for the three scales\n","        self.anchors = [\n","            [10, 13, 16, 30, 33, 23],  # Small scale anchors\n","            [30, 61, 62, 45, 59, 119],  # Medium scale anchors\n","            [116, 90, 156, 198, 373, 326]  # Large scale anchors\n","        ]\n","    def __init__(self, num_classes=11):\n","\n","        self.initialize_anchors()  # Initialize anchors for detection layers\n","        super(YOLOv5, self).__init__()\n","\n","        self.num_classes = num_classes\n","\n","        # Backbone\n","        self.conv1 = ConvBNAct(3, 64, 6, 2, 2)\n","        self.conv2 = ConvBNAct(64, 128, 3, 2, 1)\n","        self.c3_1 = C3(128, 128, 3)\n","        self.conv3 = ConvBNAct(128, 256, 3, 2, 1)\n","        self.c3_2 = C3(256, 256, 6)\n","        self.conv4 = ConvBNAct(256, 512, 3, 2, 1)\n","        self.c3_3 = C3(512, 512, 9)\n","        self.conv5 = ConvBNAct(512, 1024, 3, 2, 1)\n","        self.sppf = SPPF(1024, 1024)\n","\n","        # Neck\n","        self.c3_neck_1 = C3(1024 + 512, 512, 3)  # Adjusted to handle 1024+512=1536 channels\n","        self.c3_neck_2 = C3(512 + 256, 256, 3)\n","\n","        # Upsample layers\n","        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n","\n","        # Prediction Layers (Head)\n","        self.detect_large = nn.Conv2d(1024, 3 * (self.num_classes + 5), 1)  # Large scale detection\n","        self.detect_medium = nn.Conv2d(512, 3 * (self.num_classes + 5), 1)  # Medium scale detection\n","        self.detect_small = nn.Conv2d(256, 3 * (self.num_classes + 5), 1)  # Small scale detection\n","\n","    def forward(self, x):\n","        # Backbone\n","        x1 = self.conv1(x)\n","        x2 = self.conv2(x1)\n","        x2 = self.c3_1(x2)\n","        x3 = self.conv3(x2)\n","        x3 = self.c3_2(x3)\n","        x4 = self.conv4(x3)\n","        x4 = self.c3_3(x4)\n","        x5 = self.conv5(x4)\n","        x5 = self.sppf(x5)\n","\n","        # Large scale detection\n","        detect_large = self.detect_large(x5)  # Output size: [batch_size, (num_classes + 5), h, w]\n","\n","        # Neck\n","        x5_upsampled = self.upsample(x5)\n","        x4_concat = torch.cat((x5_upsampled, x4), dim=1)\n","        x4 = self.c3_neck_1(x4_concat)\n","\n","        # Medium scale detection\n","        detect_medium = self.detect_medium(x4)\n","\n","        x4_upsampled = self.upsample(x4)\n","        x3_concat = torch.cat((x4_upsampled, x3), dim=1)\n","        x3 = self.c3_neck_2(x3_concat)\n","\n","        # Small scale detection\n","        detect_small = self.detect_small(x3)\n","\n","        # 각 출력값을 반환할 때 바운드인포스, confidence, 클래스 분류\n","        predictions = [detect_small, detect_medium, detect_large]\n","\n","        # 바운드 박스, confidence, 클래스 정보 분류\n","        pred_boxes = [pred[..., :4] for pred in predictions]  # [x_center, y_center, w, h]\n","        pred_conf = [torch.sigmoid(pred[..., 4:5]) for pred in predictions]  # confidence (sigmoid 적용)\n","        pred_cls = [torch.sigmoid(pred[..., 5:]) for pred in predictions]    # 클래스 (sigmoid 적용)\n","\n","        return pred_boxes, pred_conf, pred_cls\n","\n","# Dataset improvements and fixes\n","class ObjectDetectionDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transforms=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n","        self.image_files = self.image_files[:len(self.image_files)//10]\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        image_path = os.path.join(self.image_dir, self.image_files[idx])\n","        image = Image.open(image_path).convert(\"RGB\")\n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        # Load label\n","        label_path = os.path.join(self.label_dir, os.path.splitext(self.image_files[idx])[0] + '.txt')\n","        boxes = []\n","        labels = []\n","        with open(label_path, 'r') as f:\n","            for line in f.readlines():\n","                label, x_center, y_center, width, height = map(float, line.strip().split())\n","                labels.append(int(label))\n","                # YOLOv5 expects [x_center, y_center, width, height] normalized\n","                boxes.append([x_center, y_center, width, height])\n","\n","        boxes = torch.tensor(boxes, dtype=torch.float32)\n","        labels = torch.tensor(labels, dtype=torch.long)\n","\n","        # YOLOv5 expects targets in the format: [class, x_center, y_center, width, height]\n","        targets = torch.cat((labels.unsqueeze(1).float(), boxes), dim=1)\n","\n","        return image, targets\n","\n","# Image transformations including normalization\n","image_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Instantiate the model and print the summary with smaller input size to fit memory constraints\n","model = YOLOv5(num_classes=11).to('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyperparameters\n","num_classes = 11\n","batch_size = 16\n","learning_rate = 0.001\n","epochs = 30\n","\n","# Optimizer and loss function\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = nn.BCEWithLogitsLoss()\n","mean_ap_metric = MeanAveragePrecision()\n","\n","# DataLoader\n","train_loader = DataLoader(ObjectDetectionDataset(\"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/train\", \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/train\", transforms=image_transforms),\n","                          batch_size=1, shuffle=True, num_workers=4, pin_memory=True)\n","val_loader = DataLoader(ObjectDetectionDataset(\"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/valid\", \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/valid\", transforms=image_transforms),\n","                        batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n","\n","def compute_loss(predictions, targets, anchors):\n","    # Computes the loss using anchor boxes for more accurate localization\n","    batch_size = targets.size(0)\n","    num_anchors = 3  # Assuming 3 anchors per detection layer\n","    targets = targets.view(batch_size, -1, 5)\n","    expanded_targets = targets.unsqueeze(1).expand(-1, num_anchors, -1, -1).contiguous().view(batch_size, -1, 5)\n","    pred_boxes, pred_conf, pred_cls = predictions\n","    anchor_tensors = [\n","        torch.tensor(anchor, device=targets.device).float().view(-1, 2) for anchor in anchors\n","    ]  # Create anchor tensors for each detection scale  # Create anchor tensors for each detection scale\n","    box_loss = sum(\n","        F.mse_loss(pred_box.view(batch_size, -1, 4), expanded_targets[..., 1:5]) for pred_box, anchor in zip(pred_boxes, anchor_tensors)\n","    )  # Calculate box loss using anchors\n","    conf_loss = sum(criterion(pred_conf[i].view(batch_size, -1, 1), expanded_targets[..., 0:1]) for i in range(len(pred_conf)))\n","    cls_loss = sum(criterion(pred_cls[i].view(batch_size, -1, num_classes), expanded_targets[..., 0:1].expand_as(pred_cls[i].view(batch_size, -1, num_classes))) for i in range(len(pred_cls)))\n","    return box_loss + conf_loss + cls_loss\n","\n","# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n","        images, targets = images.to(device), targets.to(device)\n","\n","        print(images[0].shape) # torch.Size([3, 512, 512])\n","        print(targets[0].shape) # torch.Size([1, 5]) # class, x_center, y_center, width, height\n","\n","        optimizer.zero_grad()\n","        predictions = model(images)\n","        print(len(predictions)) # 3\n","\n","\n","        print(len(predictions[0])) # 3\n","        print(predictions[0])\n","        print(len(predictions[1])) # 3\n","        print(len(predictions[2])) # 3\n","\n","        loss = compute_loss(predictions, targets, model.anchors)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    print(f\"Epoch {epoch + 1}, Training Loss: {train_loss / len(train_loader)}\")\n","\n","    # Validation and mAP calculation\n","    model.eval()\n","    mean_ap_metric.reset()\n","    with torch.no_grad():\n","        for images, targets in val_loader:\n","            images, targets = images.to(device), targets.to(device)\n","            pred_boxes, pred_conf, pred_cls = model(images)\n","            detections = []\n","            for i in range(len(pred_boxes)):\n","                for j in range(len(pred_boxes[i])):\n","                    detection = torch.cat((pred_boxes[i][j], pred_conf[i][j], pred_cls[i][j]), dim=-1)\n","                    detections.append(detection)\n","            mean_ap_metric.update(detections, targets)\n","\n","    mAP = mean_ap_metric.compute()\n","    print(f\"Epoch {epoch + 1}, Validation mAP: {mAP}\")\n","\n","# Print summary\n","from torchsummary import summary\n","summary(model, (3, 512, 512))\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731377122650,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"P24WdNkO-lhP"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731377122650,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"g32GDcQsgYNf"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"3hMAwZPAOwPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VrzH7Ko9OwLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2K2o4DenOwIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5ldiluK6OwEx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PNEm7BX4OwBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wMLOM35bOv9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ieuwB2bOv6j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YKfhi6jqOv3O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"01OghUSrOvz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uSze15G1Ovwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4IG9ouFHOvs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731377122650,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"DF6E1Ry3S2XJ"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["----------------------"],"metadata":{"id":"u7WlfDn9S2au"}},{"cell_type":"markdown","source":["# 여기부터"],"metadata":{"id":"SBaOiNDEQza3"}},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7219,"status":"ok","timestamp":1731377444064,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"cnlsdlKbS2eJ"},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# from torch.utils.data import DataLoader\n","# from torchvision import transforms\n","# import numpy as np\n","# import os\n","\n","# # 모델이 GPU를 사용할 수 있는지 확인\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1731377444064,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"Bw9DqtHFUdyX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1731377444064,"user":{"displayName":"김형준","userId":"05139800887759179481"},"user_tz":-540},"id":"LTsdsjt7S2hQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"70H0pn_xUbFx","executionInfo":{"status":"ok","timestamp":1731377444064,"user_tz":-540,"elapsed":8,"user":{"displayName":"김형준","userId":"05139800887759179481"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9n8_HIudUbKD","executionInfo":{"status":"ok","timestamp":1731377444064,"user_tz":-540,"elapsed":7,"user":{"displayName":"김형준","userId":"05139800887759179481"}}},"outputs":[],"source":[]},{"cell_type":"code","source":["# torch.cuda.empty_cache()\n","\n","# import gc\n","# gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9CPmMithS1TU","executionInfo":{"status":"ok","timestamp":1731377444064,"user_tz":-540,"elapsed":7,"user":{"displayName":"김형준","userId":"05139800887759179481"}},"outputId":"f38e2077-e420-4ac7-8615-cddfb39db554"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["42"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","\n","# class ConvBNAct(nn.Module):\n","#     def __init__(self, in_channels, out_channels, kernel_size, stride, padding, act=True):\n","#         super(ConvBNAct, self).__init__()\n","#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n","#         self.bn = nn.BatchNorm2d(out_channels)\n","#         self.act = nn.SiLU() if act else nn.Identity()\n","\n","#     def forward(self, x):\n","#         return self.act(self.bn(self.conv(x)))\n","\n","# class BottleNeck(nn.Module):\n","#     def __init__(self, in_channels, out_channels, shortcut=True):\n","#         super(BottleNeck, self).__init__()\n","#         self.conv1 = ConvBNAct(in_channels, out_channels, 1, 1, 0)\n","#         self.conv2 = ConvBNAct(out_channels, out_channels, 3, 1, 1)\n","#         self.shortcut = shortcut and in_channels == out_channels\n","\n","#     def forward(self, x):\n","#         out = self.conv2(self.conv1(x))\n","#         return x + out if self.shortcut else out\n","\n","# class C3(nn.Module):\n","#     def __init__(self, in_channels, out_channels, n=3):\n","#         super(C3, self).__init__()\n","#         self.conv1 = ConvBNAct(in_channels, out_channels // 2, 1, 1, 0)\n","#         self.conv2 = ConvBNAct(in_channels, out_channels // 2, 1, 1, 0)\n","#         self.conv3 = ConvBNAct(out_channels, out_channels, 1, 1, 0)\n","#         self.bottlenecks = nn.Sequential(*[BottleNeck(out_channels // 2, out_channels // 2) for _ in range(n)])\n","\n","#     def forward(self, x):\n","#         y1 = self.bottlenecks(self.conv1(x))\n","#         y2 = self.conv2(x)\n","#         return self.conv3(torch.cat((y1, y2), dim=1))\n","\n","# class SPPF(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super(SPPF, self).__init__()\n","#         self.conv1 = ConvBNAct(in_channels, out_channels, 1, 1, 0)\n","#         self.conv2 = ConvBNAct(out_channels * 4, out_channels, 1, 1, 0)\n","\n","#     def forward(self, x):\n","#         x = self.conv1(x)\n","#         y1 = F.max_pool2d(x, 5, stride=1, padding=2)\n","#         y2 = F.max_pool2d(x, 9, stride=1, padding=4)\n","#         y3 = F.max_pool2d(x, 13, stride=1, padding=6)\n","#         return self.conv2(torch.cat([x, y1, y2, y3], dim=1))\n","\n","# class YOLOv5(nn.Module):\n","#     def __init__(self, num_classes=11):  # Update with the correct number of classes\n","#         super(YOLOv5, self).__init__()\n","\n","#         self.num_classes = num_classes\n","\n","#         # Backbone\n","#         self.conv1 = ConvBNAct(3, 64, 6, 2, 2)\n","#         self.conv2 = ConvBNAct(64, 128, 3, 2, 1)\n","#         self.c3_1 = C3(128, 128, 3)\n","#         self.conv3 = ConvBNAct(128, 256, 3, 2, 1)\n","#         self.c3_2 = C3(256, 256, 6)\n","#         self.conv4 = ConvBNAct(256, 512, 3, 2, 1)\n","#         self.c3_3 = C3(512, 512, 9)\n","#         self.conv5 = ConvBNAct(512, 1024, 3, 2, 1)\n","#         self.sppf = SPPF(1024, 1024)\n","\n","#         # Neck\n","#         self.c3_neck_1 = C3(1024 + 512, 512, 3)  # Adjusted to handle 1024+512=1536 channels\n","#         self.c3_neck_2 = C3(512 + 256, 256, 3)\n","\n","#         # Upsample layers\n","#         self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n","\n","#         # Prediction Layers (Head)\n","#         self.detect_large = nn.Conv2d(1024, 3 * (self.num_classes + 5), 1)  # Large scale detection\n","#         self.detect_medium = nn.Conv2d(512, 3 * (self.num_classes + 5), 1)  # Medium scale detection\n","#         self.detect_small = nn.Conv2d(256, 3 * (self.num_classes + 5), 1)  # Small scale detection\n","\n","\n","#     def forward(self, x):\n","#         # Backbone\n","#         x1 = self.conv1(x)\n","#         x2 = self.conv2(x1)\n","#         x2 = self.c3_1(x2)\n","#         x3 = self.conv3(x2)\n","#         x3 = self.c3_2(x3)\n","#         x4 = self.conv4(x3)\n","#         x4 = self.c3_3(x4)\n","#         x5 = self.conv5(x4)\n","#         x5 = self.sppf(x5)\n","\n","#         # Large scale detection\n","#         detect_large = self.detect_large(x5)  # Output size: [batch_size, (num_classes + 5), h, w]\n","\n","#         # Neck\n","#         x5_upsampled = self.upsample(x5)\n","#         x4_concat = torch.cat((x5_upsampled, x4), dim=1)\n","#         x4 = self.c3_neck_1(x4_concat)\n","\n","#         # Medium scale detection\n","#         detect_medium = self.detect_medium(x4)\n","\n","#         x4_upsampled = self.upsample(x4)\n","#         x3_concat = torch.cat((x4_upsampled, x3), dim=1)\n","#         x3 = self.c3_neck_2(x3_concat)\n","\n","#         # Small scale detection\n","#         detect_small = self.detect_small(x3)\n","\n","#         # 각 출력값을 반환할 때 바운딩 박스, confidence, 클래스 분리\n","#         predictions = [detect_small, detect_medium, detect_large]\n","\n","#         # 바운딩 박스, confidence, 클래스 정보 분리\n","#         pred_boxes = [pred[..., :4] for pred in predictions]  # [x_center, y_center, w, h]\n","#         pred_conf = [torch.sigmoid(pred[..., 4:5]) for pred in predictions]  # confidence (sigmoid 적용)\n","#         pred_cls = [torch.sigmoid(pred[..., 5:]) for pred in predictions]    # 클래스 (sigmoid 적용)\n","\n","#         return pred_boxes, pred_conf, pred_cls\n","\n","\n","# # To display model summary\n","# from torchsummary import summary\n","\n","# # Instantiate the model and print the summary with smaller input size to fit memory constraints\n","# model = YOLOv5(num_classes=11).to('cuda' if torch.cuda.is_available() else 'cpu')\n","# summary(model, (3, 512, 512))\n","\n","# anchors = [\n","#     [10, 13, 16, 30, 33, 23],\n","#     [30, 61, 62, 45, 59, 119],\n","#     [116, 90, 156, 198, 373, 326]\n","# ]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXJNCyY_F0iT","executionInfo":{"status":"ok","timestamp":1731377445712,"user_tz":-540,"elapsed":1653,"user":{"displayName":"김형준","userId":"05139800887759179481"}},"outputId":"38a7b75d-af46-4a48-96de-bff4f0e99071"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 256, 256]           6,912\n","       BatchNorm2d-2         [-1, 64, 256, 256]             128\n","              SiLU-3         [-1, 64, 256, 256]               0\n","         ConvBNAct-4         [-1, 64, 256, 256]               0\n","            Conv2d-5        [-1, 128, 128, 128]          73,728\n","       BatchNorm2d-6        [-1, 128, 128, 128]             256\n","              SiLU-7        [-1, 128, 128, 128]               0\n","         ConvBNAct-8        [-1, 128, 128, 128]               0\n","            Conv2d-9         [-1, 64, 128, 128]           8,192\n","      BatchNorm2d-10         [-1, 64, 128, 128]             128\n","             SiLU-11         [-1, 64, 128, 128]               0\n","        ConvBNAct-12         [-1, 64, 128, 128]               0\n","           Conv2d-13         [-1, 64, 128, 128]           4,096\n","      BatchNorm2d-14         [-1, 64, 128, 128]             128\n","             SiLU-15         [-1, 64, 128, 128]               0\n","        ConvBNAct-16         [-1, 64, 128, 128]               0\n","           Conv2d-17         [-1, 64, 128, 128]          36,864\n","      BatchNorm2d-18         [-1, 64, 128, 128]             128\n","             SiLU-19         [-1, 64, 128, 128]               0\n","        ConvBNAct-20         [-1, 64, 128, 128]               0\n","       BottleNeck-21         [-1, 64, 128, 128]               0\n","           Conv2d-22         [-1, 64, 128, 128]           4,096\n","      BatchNorm2d-23         [-1, 64, 128, 128]             128\n","             SiLU-24         [-1, 64, 128, 128]               0\n","        ConvBNAct-25         [-1, 64, 128, 128]               0\n","           Conv2d-26         [-1, 64, 128, 128]          36,864\n","      BatchNorm2d-27         [-1, 64, 128, 128]             128\n","             SiLU-28         [-1, 64, 128, 128]               0\n","        ConvBNAct-29         [-1, 64, 128, 128]               0\n","       BottleNeck-30         [-1, 64, 128, 128]               0\n","           Conv2d-31         [-1, 64, 128, 128]           4,096\n","      BatchNorm2d-32         [-1, 64, 128, 128]             128\n","             SiLU-33         [-1, 64, 128, 128]               0\n","        ConvBNAct-34         [-1, 64, 128, 128]               0\n","           Conv2d-35         [-1, 64, 128, 128]          36,864\n","      BatchNorm2d-36         [-1, 64, 128, 128]             128\n","             SiLU-37         [-1, 64, 128, 128]               0\n","        ConvBNAct-38         [-1, 64, 128, 128]               0\n","       BottleNeck-39         [-1, 64, 128, 128]               0\n","           Conv2d-40         [-1, 64, 128, 128]           8,192\n","      BatchNorm2d-41         [-1, 64, 128, 128]             128\n","             SiLU-42         [-1, 64, 128, 128]               0\n","        ConvBNAct-43         [-1, 64, 128, 128]               0\n","           Conv2d-44        [-1, 128, 128, 128]          16,384\n","      BatchNorm2d-45        [-1, 128, 128, 128]             256\n","             SiLU-46        [-1, 128, 128, 128]               0\n","        ConvBNAct-47        [-1, 128, 128, 128]               0\n","               C3-48        [-1, 128, 128, 128]               0\n","           Conv2d-49          [-1, 256, 64, 64]         294,912\n","      BatchNorm2d-50          [-1, 256, 64, 64]             512\n","             SiLU-51          [-1, 256, 64, 64]               0\n","        ConvBNAct-52          [-1, 256, 64, 64]               0\n","           Conv2d-53          [-1, 128, 64, 64]          32,768\n","      BatchNorm2d-54          [-1, 128, 64, 64]             256\n","             SiLU-55          [-1, 128, 64, 64]               0\n","        ConvBNAct-56          [-1, 128, 64, 64]               0\n","           Conv2d-57          [-1, 128, 64, 64]          16,384\n","      BatchNorm2d-58          [-1, 128, 64, 64]             256\n","             SiLU-59          [-1, 128, 64, 64]               0\n","        ConvBNAct-60          [-1, 128, 64, 64]               0\n","           Conv2d-61          [-1, 128, 64, 64]         147,456\n","      BatchNorm2d-62          [-1, 128, 64, 64]             256\n","             SiLU-63          [-1, 128, 64, 64]               0\n","        ConvBNAct-64          [-1, 128, 64, 64]               0\n","       BottleNeck-65          [-1, 128, 64, 64]               0\n","           Conv2d-66          [-1, 128, 64, 64]          16,384\n","      BatchNorm2d-67          [-1, 128, 64, 64]             256\n","             SiLU-68          [-1, 128, 64, 64]               0\n","        ConvBNAct-69          [-1, 128, 64, 64]               0\n","           Conv2d-70          [-1, 128, 64, 64]         147,456\n","      BatchNorm2d-71          [-1, 128, 64, 64]             256\n","             SiLU-72          [-1, 128, 64, 64]               0\n","        ConvBNAct-73          [-1, 128, 64, 64]               0\n","       BottleNeck-74          [-1, 128, 64, 64]               0\n","           Conv2d-75          [-1, 128, 64, 64]          16,384\n","      BatchNorm2d-76          [-1, 128, 64, 64]             256\n","             SiLU-77          [-1, 128, 64, 64]               0\n","        ConvBNAct-78          [-1, 128, 64, 64]               0\n","           Conv2d-79          [-1, 128, 64, 64]         147,456\n","      BatchNorm2d-80          [-1, 128, 64, 64]             256\n","             SiLU-81          [-1, 128, 64, 64]               0\n","        ConvBNAct-82          [-1, 128, 64, 64]               0\n","       BottleNeck-83          [-1, 128, 64, 64]               0\n","           Conv2d-84          [-1, 128, 64, 64]          16,384\n","      BatchNorm2d-85          [-1, 128, 64, 64]             256\n","             SiLU-86          [-1, 128, 64, 64]               0\n","        ConvBNAct-87          [-1, 128, 64, 64]               0\n","           Conv2d-88          [-1, 128, 64, 64]         147,456\n","      BatchNorm2d-89          [-1, 128, 64, 64]             256\n","             SiLU-90          [-1, 128, 64, 64]               0\n","        ConvBNAct-91          [-1, 128, 64, 64]               0\n","       BottleNeck-92          [-1, 128, 64, 64]               0\n","           Conv2d-93          [-1, 128, 64, 64]          16,384\n","      BatchNorm2d-94          [-1, 128, 64, 64]             256\n","             SiLU-95          [-1, 128, 64, 64]               0\n","        ConvBNAct-96          [-1, 128, 64, 64]               0\n","           Conv2d-97          [-1, 128, 64, 64]         147,456\n","      BatchNorm2d-98          [-1, 128, 64, 64]             256\n","             SiLU-99          [-1, 128, 64, 64]               0\n","       ConvBNAct-100          [-1, 128, 64, 64]               0\n","      BottleNeck-101          [-1, 128, 64, 64]               0\n","          Conv2d-102          [-1, 128, 64, 64]          16,384\n","     BatchNorm2d-103          [-1, 128, 64, 64]             256\n","            SiLU-104          [-1, 128, 64, 64]               0\n","       ConvBNAct-105          [-1, 128, 64, 64]               0\n","          Conv2d-106          [-1, 128, 64, 64]         147,456\n","     BatchNorm2d-107          [-1, 128, 64, 64]             256\n","            SiLU-108          [-1, 128, 64, 64]               0\n","       ConvBNAct-109          [-1, 128, 64, 64]               0\n","      BottleNeck-110          [-1, 128, 64, 64]               0\n","          Conv2d-111          [-1, 128, 64, 64]          32,768\n","     BatchNorm2d-112          [-1, 128, 64, 64]             256\n","            SiLU-113          [-1, 128, 64, 64]               0\n","       ConvBNAct-114          [-1, 128, 64, 64]               0\n","          Conv2d-115          [-1, 256, 64, 64]          65,536\n","     BatchNorm2d-116          [-1, 256, 64, 64]             512\n","            SiLU-117          [-1, 256, 64, 64]               0\n","       ConvBNAct-118          [-1, 256, 64, 64]               0\n","              C3-119          [-1, 256, 64, 64]               0\n","          Conv2d-120          [-1, 512, 32, 32]       1,179,648\n","     BatchNorm2d-121          [-1, 512, 32, 32]           1,024\n","            SiLU-122          [-1, 512, 32, 32]               0\n","       ConvBNAct-123          [-1, 512, 32, 32]               0\n","          Conv2d-124          [-1, 256, 32, 32]         131,072\n","     BatchNorm2d-125          [-1, 256, 32, 32]             512\n","            SiLU-126          [-1, 256, 32, 32]               0\n","       ConvBNAct-127          [-1, 256, 32, 32]               0\n","          Conv2d-128          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-129          [-1, 256, 32, 32]             512\n","            SiLU-130          [-1, 256, 32, 32]               0\n","       ConvBNAct-131          [-1, 256, 32, 32]               0\n","          Conv2d-132          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-133          [-1, 256, 32, 32]             512\n","            SiLU-134          [-1, 256, 32, 32]               0\n","       ConvBNAct-135          [-1, 256, 32, 32]               0\n","      BottleNeck-136          [-1, 256, 32, 32]               0\n","          Conv2d-137          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-138          [-1, 256, 32, 32]             512\n","            SiLU-139          [-1, 256, 32, 32]               0\n","       ConvBNAct-140          [-1, 256, 32, 32]               0\n","          Conv2d-141          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-142          [-1, 256, 32, 32]             512\n","            SiLU-143          [-1, 256, 32, 32]               0\n","       ConvBNAct-144          [-1, 256, 32, 32]               0\n","      BottleNeck-145          [-1, 256, 32, 32]               0\n","          Conv2d-146          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-147          [-1, 256, 32, 32]             512\n","            SiLU-148          [-1, 256, 32, 32]               0\n","       ConvBNAct-149          [-1, 256, 32, 32]               0\n","          Conv2d-150          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-151          [-1, 256, 32, 32]             512\n","            SiLU-152          [-1, 256, 32, 32]               0\n","       ConvBNAct-153          [-1, 256, 32, 32]               0\n","      BottleNeck-154          [-1, 256, 32, 32]               0\n","          Conv2d-155          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-156          [-1, 256, 32, 32]             512\n","            SiLU-157          [-1, 256, 32, 32]               0\n","       ConvBNAct-158          [-1, 256, 32, 32]               0\n","          Conv2d-159          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-160          [-1, 256, 32, 32]             512\n","            SiLU-161          [-1, 256, 32, 32]               0\n","       ConvBNAct-162          [-1, 256, 32, 32]               0\n","      BottleNeck-163          [-1, 256, 32, 32]               0\n","          Conv2d-164          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-165          [-1, 256, 32, 32]             512\n","            SiLU-166          [-1, 256, 32, 32]               0\n","       ConvBNAct-167          [-1, 256, 32, 32]               0\n","          Conv2d-168          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-169          [-1, 256, 32, 32]             512\n","            SiLU-170          [-1, 256, 32, 32]               0\n","       ConvBNAct-171          [-1, 256, 32, 32]               0\n","      BottleNeck-172          [-1, 256, 32, 32]               0\n","          Conv2d-173          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-174          [-1, 256, 32, 32]             512\n","            SiLU-175          [-1, 256, 32, 32]               0\n","       ConvBNAct-176          [-1, 256, 32, 32]               0\n","          Conv2d-177          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-178          [-1, 256, 32, 32]             512\n","            SiLU-179          [-1, 256, 32, 32]               0\n","       ConvBNAct-180          [-1, 256, 32, 32]               0\n","      BottleNeck-181          [-1, 256, 32, 32]               0\n","          Conv2d-182          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-183          [-1, 256, 32, 32]             512\n","            SiLU-184          [-1, 256, 32, 32]               0\n","       ConvBNAct-185          [-1, 256, 32, 32]               0\n","          Conv2d-186          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-187          [-1, 256, 32, 32]             512\n","            SiLU-188          [-1, 256, 32, 32]               0\n","       ConvBNAct-189          [-1, 256, 32, 32]               0\n","      BottleNeck-190          [-1, 256, 32, 32]               0\n","          Conv2d-191          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-192          [-1, 256, 32, 32]             512\n","            SiLU-193          [-1, 256, 32, 32]               0\n","       ConvBNAct-194          [-1, 256, 32, 32]               0\n","          Conv2d-195          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-196          [-1, 256, 32, 32]             512\n","            SiLU-197          [-1, 256, 32, 32]               0\n","       ConvBNAct-198          [-1, 256, 32, 32]               0\n","      BottleNeck-199          [-1, 256, 32, 32]               0\n","          Conv2d-200          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-201          [-1, 256, 32, 32]             512\n","            SiLU-202          [-1, 256, 32, 32]               0\n","       ConvBNAct-203          [-1, 256, 32, 32]               0\n","          Conv2d-204          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-205          [-1, 256, 32, 32]             512\n","            SiLU-206          [-1, 256, 32, 32]               0\n","       ConvBNAct-207          [-1, 256, 32, 32]               0\n","      BottleNeck-208          [-1, 256, 32, 32]               0\n","          Conv2d-209          [-1, 256, 32, 32]         131,072\n","     BatchNorm2d-210          [-1, 256, 32, 32]             512\n","            SiLU-211          [-1, 256, 32, 32]               0\n","       ConvBNAct-212          [-1, 256, 32, 32]               0\n","          Conv2d-213          [-1, 512, 32, 32]         262,144\n","     BatchNorm2d-214          [-1, 512, 32, 32]           1,024\n","            SiLU-215          [-1, 512, 32, 32]               0\n","       ConvBNAct-216          [-1, 512, 32, 32]               0\n","              C3-217          [-1, 512, 32, 32]               0\n","          Conv2d-218         [-1, 1024, 16, 16]       4,718,592\n","     BatchNorm2d-219         [-1, 1024, 16, 16]           2,048\n","            SiLU-220         [-1, 1024, 16, 16]               0\n","       ConvBNAct-221         [-1, 1024, 16, 16]               0\n","          Conv2d-222         [-1, 1024, 16, 16]       1,048,576\n","     BatchNorm2d-223         [-1, 1024, 16, 16]           2,048\n","            SiLU-224         [-1, 1024, 16, 16]               0\n","       ConvBNAct-225         [-1, 1024, 16, 16]               0\n","          Conv2d-226         [-1, 1024, 16, 16]       4,194,304\n","     BatchNorm2d-227         [-1, 1024, 16, 16]           2,048\n","            SiLU-228         [-1, 1024, 16, 16]               0\n","       ConvBNAct-229         [-1, 1024, 16, 16]               0\n","            SPPF-230         [-1, 1024, 16, 16]               0\n","          Conv2d-231           [-1, 48, 16, 16]          49,200\n","        Upsample-232         [-1, 1024, 32, 32]               0\n","          Conv2d-233          [-1, 256, 32, 32]         393,216\n","     BatchNorm2d-234          [-1, 256, 32, 32]             512\n","            SiLU-235          [-1, 256, 32, 32]               0\n","       ConvBNAct-236          [-1, 256, 32, 32]               0\n","          Conv2d-237          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-238          [-1, 256, 32, 32]             512\n","            SiLU-239          [-1, 256, 32, 32]               0\n","       ConvBNAct-240          [-1, 256, 32, 32]               0\n","          Conv2d-241          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-242          [-1, 256, 32, 32]             512\n","            SiLU-243          [-1, 256, 32, 32]               0\n","       ConvBNAct-244          [-1, 256, 32, 32]               0\n","      BottleNeck-245          [-1, 256, 32, 32]               0\n","          Conv2d-246          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-247          [-1, 256, 32, 32]             512\n","            SiLU-248          [-1, 256, 32, 32]               0\n","       ConvBNAct-249          [-1, 256, 32, 32]               0\n","          Conv2d-250          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-251          [-1, 256, 32, 32]             512\n","            SiLU-252          [-1, 256, 32, 32]               0\n","       ConvBNAct-253          [-1, 256, 32, 32]               0\n","      BottleNeck-254          [-1, 256, 32, 32]               0\n","          Conv2d-255          [-1, 256, 32, 32]          65,536\n","     BatchNorm2d-256          [-1, 256, 32, 32]             512\n","            SiLU-257          [-1, 256, 32, 32]               0\n","       ConvBNAct-258          [-1, 256, 32, 32]               0\n","          Conv2d-259          [-1, 256, 32, 32]         589,824\n","     BatchNorm2d-260          [-1, 256, 32, 32]             512\n","            SiLU-261          [-1, 256, 32, 32]               0\n","       ConvBNAct-262          [-1, 256, 32, 32]               0\n","      BottleNeck-263          [-1, 256, 32, 32]               0\n","          Conv2d-264          [-1, 256, 32, 32]         393,216\n","     BatchNorm2d-265          [-1, 256, 32, 32]             512\n","            SiLU-266          [-1, 256, 32, 32]               0\n","       ConvBNAct-267          [-1, 256, 32, 32]               0\n","          Conv2d-268          [-1, 512, 32, 32]         262,144\n","     BatchNorm2d-269          [-1, 512, 32, 32]           1,024\n","            SiLU-270          [-1, 512, 32, 32]               0\n","       ConvBNAct-271          [-1, 512, 32, 32]               0\n","              C3-272          [-1, 512, 32, 32]               0\n","          Conv2d-273           [-1, 48, 32, 32]          24,624\n","        Upsample-274          [-1, 512, 64, 64]               0\n","          Conv2d-275          [-1, 128, 64, 64]          98,304\n","     BatchNorm2d-276          [-1, 128, 64, 64]             256\n","            SiLU-277          [-1, 128, 64, 64]               0\n","       ConvBNAct-278          [-1, 128, 64, 64]               0\n","          Conv2d-279          [-1, 128, 64, 64]          16,384\n","     BatchNorm2d-280          [-1, 128, 64, 64]             256\n","            SiLU-281          [-1, 128, 64, 64]               0\n","       ConvBNAct-282          [-1, 128, 64, 64]               0\n","          Conv2d-283          [-1, 128, 64, 64]         147,456\n","     BatchNorm2d-284          [-1, 128, 64, 64]             256\n","            SiLU-285          [-1, 128, 64, 64]               0\n","       ConvBNAct-286          [-1, 128, 64, 64]               0\n","      BottleNeck-287          [-1, 128, 64, 64]               0\n","          Conv2d-288          [-1, 128, 64, 64]          16,384\n","     BatchNorm2d-289          [-1, 128, 64, 64]             256\n","            SiLU-290          [-1, 128, 64, 64]               0\n","       ConvBNAct-291          [-1, 128, 64, 64]               0\n","          Conv2d-292          [-1, 128, 64, 64]         147,456\n","     BatchNorm2d-293          [-1, 128, 64, 64]             256\n","            SiLU-294          [-1, 128, 64, 64]               0\n","       ConvBNAct-295          [-1, 128, 64, 64]               0\n","      BottleNeck-296          [-1, 128, 64, 64]               0\n","          Conv2d-297          [-1, 128, 64, 64]          16,384\n","     BatchNorm2d-298          [-1, 128, 64, 64]             256\n","            SiLU-299          [-1, 128, 64, 64]               0\n","       ConvBNAct-300          [-1, 128, 64, 64]               0\n","          Conv2d-301          [-1, 128, 64, 64]         147,456\n","     BatchNorm2d-302          [-1, 128, 64, 64]             256\n","            SiLU-303          [-1, 128, 64, 64]               0\n","       ConvBNAct-304          [-1, 128, 64, 64]               0\n","      BottleNeck-305          [-1, 128, 64, 64]               0\n","          Conv2d-306          [-1, 128, 64, 64]          98,304\n","     BatchNorm2d-307          [-1, 128, 64, 64]             256\n","            SiLU-308          [-1, 128, 64, 64]               0\n","       ConvBNAct-309          [-1, 128, 64, 64]               0\n","          Conv2d-310          [-1, 256, 64, 64]          65,536\n","     BatchNorm2d-311          [-1, 256, 64, 64]             512\n","            SiLU-312          [-1, 256, 64, 64]               0\n","       ConvBNAct-313          [-1, 256, 64, 64]               0\n","              C3-314          [-1, 256, 64, 64]               0\n","          Conv2d-315           [-1, 48, 64, 64]          12,336\n","================================================================\n","Total params: 23,095,824\n","Trainable params: 23,095,824\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 3.00\n","Forward/backward pass size (MB): 1407.97\n","Params size (MB): 88.10\n","Estimated Total Size (MB): 1499.07\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# torch.cuda.empty_cache()\n","\n","# import gc\n","# gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lffN7uKTU-Ls","executionInfo":{"status":"ok","timestamp":1731378939980,"user_tz":-540,"elapsed":309,"user":{"displayName":"김형준","userId":"05139800887759179481"}},"outputId":"0dd0d6df-1e35-417b-f385-bb5868079040"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":[],"metadata":{"id":"vlbQILFkaspf","executionInfo":{"status":"ok","timestamp":1731377925834,"user_tz":-540,"elapsed":2,"user":{"displayName":"김형준","userId":"05139800887759179481"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# import os\n","# import torch\n","# import torch.optim as optim\n","# import torch.nn as nn\n","# from torch.utils.data import DataLoader, Dataset\n","# from torchvision import transforms\n","# from tqdm import tqdm\n","# import numpy as np\n","# import torchvision.ops.boxes as box_ops\n","# from PIL import Image\n","\n","# # Hyperparameters\n","# num_classes = 11\n","# batch_size = 16\n","# learning_rate = 0.001\n","# epochs = 30\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# # # Dataset paths\n","# # train_image_paths = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/ultralytics_dataset/images/train\"\n","# # train_labels = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/ultralytics_dataset/labels/train\"\n","# # val_image_paths = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/ultralytics_dataset/images/valid\"\n","# # val_labels = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/ultralytics_dataset/labels/valid\"\n","\n","# # test_image_paths = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/ultralytics_dataset/images/test\"\n","# # test_labels = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/ultralytics_dataset/labels/test\"\n","\n","\n","# # Dataset paths\n","# train_image_paths = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/train\"\n","# train_labels = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/train\"\n","# val_image_paths = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/valid\"\n","# val_labels = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/valid\"\n","\n","# test_image_paths = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/images/test\"\n","# test_labels = \"/content/drive/MyDrive/like_lion/Computer_Vision/Object_Detection_Project/yolov5_dataset/labels/test\"\n","\n","# # Custom ObjectDetectionDataset\n","# class ObjectDetectionDataset(Dataset):\n","#     def __init__(self, image_dir, label_dir, transforms=None):\n","#         self.image_dir = image_dir\n","#         self.label_dir = label_dir\n","#         self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n","#         self.transforms = transforms\n","\n","#     def __len__(self):\n","#         return len(self.image_files)\n","\n","#     def __getitem__(self, idx):\n","#         # Load image\n","#         image_path = os.path.join(self.image_dir, self.image_files[idx])\n","#         image = Image.open(image_path).convert(\"RGB\")\n","#         if self.transforms:\n","#             image = self.transforms(image)\n","\n","#         # Load label\n","#         label_path = os.path.join(self.label_dir, self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt'))\n","#         boxes = []\n","#         labels = []\n","#         with open(label_path, 'r') as f:\n","#             for line in f.readlines():\n","#                 label, x_center, y_center, width, height = map(float, line.strip().split())\n","#                 labels.append(int(label))\n","#                 # YOLOv5 expects [x_center, y_center, width, height] normalized\n","#                 boxes.append([x_center, y_center, width, height])\n","\n","#         boxes = torch.tensor(boxes, dtype=torch.float32)\n","#         labels = torch.tensor(labels, dtype=torch.long)\n","\n","#         # YOLOv5 expects targets in the format: [class, x_center, y_center, width, height]\n","#         targets = torch.cat((labels.unsqueeze(1).float(), boxes), dim=1)\n","\n","#         return image, targets\n","\n","# # Image transformations including normalization\n","# image_transforms = transforms.Compose([\n","#     transforms.ToTensor(),\n","#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","# ])\n","\n","# # Load datasets\n","# train_dataset = ObjectDetectionDataset(train_image_paths, train_labels, transforms=image_transforms)\n","# val_dataset = ObjectDetectionDataset(val_image_paths, val_labels, transforms=image_transforms)\n","# test_dataset = ObjectDetectionDataset(test_image_paths, test_labels, transforms=image_transforms)\n","\n","# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n","# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n","\n","# # Load model\n","# model = YOLOv5(num_classes=num_classes).to(device)\n","\n","# # Improved GIoU Loss and Total Loss Function\n","# class ImprovedGIoULoss(nn.Module):\n","#     def __init__(self):\n","#         super(ImprovedGIoULoss, self).__init__()\n","#         self.bce_conf = nn.BCEWithLogitsLoss()  # For confidence score\n","#         self.ce_cls = nn.CrossEntropyLoss()  # For class prediction\n","\n","#     def forward(self, pred_boxes, pred_conf, pred_cls, targets):\n","#         total_loss = 0.0\n","#         localization_loss = 0.0\n","#         confidence_loss = 0.0\n","#         classification_loss = 0.0\n","\n","#         for pred_box, pred_conf_single, pred_cls_single, target in zip(pred_boxes, pred_conf, pred_cls, targets):\n","#             if target.size(0) == 0:\n","#                 continue\n","\n","#             pred_box = pred_box.view(-1, 4)  # Flatten predicted boxes to [num_predictions, 4]\n","#             target_boxes = target[:, 1:5]  # Extract target boxes [num_targets, 4]\n","#             target_labels = target[:, 0]  # Extract class labels\n","\n","#             # GIoU Loss for localization\n","#             if pred_box.size(0) != 0 and target_boxes.size(0) != 0:\n","#                 with torch.no_grad():  # Reduce memory usage during GIoU calculation\n","#                     iou = box_ops.box_iou(pred_box, target_boxes)[0]\n","#                     enclosing_x1 = torch.min(pred_box[:, 0].unsqueeze(1), target_boxes[:, 0])\n","#                     enclosing_y1 = torch.min(pred_box[:, 1].unsqueeze(1), target_boxes[:, 1])\n","#                     enclosing_x2 = torch.max(pred_box[:, 2].unsqueeze(1), target_boxes[:, 2])\n","#                     enclosing_y2 = torch.max(pred_box[:, 3].unsqueeze(1), target_boxes[:, 3])\n","#                     enclosing_area = (enclosing_x2 - enclosing_x1).clamp(min=1e-6) * (enclosing_y2 - enclosing_y1).clamp(min=1e-6)\n","#                     pred_box_area = (pred_box[:, 2] - pred_box[:, 0]).clamp(min=1e-6) * (pred_box[:, 3] - pred_box[:, 1]).clamp(min=1e-6)\n","#                     target_box_area = (target_boxes[:, 2] - target_boxes[:, 0]).clamp(min=1e-6) * (target_boxes[:, 3] - target_boxes[:, 1]).clamp(min=1e-6)\n","#                 giou = iou - ((enclosing_area - pred_box_area - target_box_area) / enclosing_area).clamp(min=0)\n","#                 localization_loss += (1 - giou).mean()\n","\n","#             # Confidence Loss\n","#             if pred_conf_single is not None:\n","#                 conf_target = torch.zeros_like(pred_conf_single)  # Default to no object\n","#                 conf_target[..., 0] = 1  # Marking positive where objects are present\n","#                 confidence_loss += self.bce_conf(pred_conf_single.view(-1), conf_target.view(-1))\n","\n","#             # Classification Loss\n","#             if pred_cls_single is not None:\n","#                 if pred_cls_single.numel() == 0:\n","#                     continue\n","#                 pred_cls_single = pred_cls_single.view(-1, num_classes)  # Ensure predicted classes are properly shaped\n","#                 target_labels = target_labels.long()  # Convert target_labels to long for CrossEntropyLoss\n","#                 target_labels = target_labels[:pred_cls_single.size(0)]  # Match size if needed\n","#                 classification_loss += self.ce_cls(pred_cls_single, target_labels)\n","\n","#         total_loss = localization_loss + confidence_loss + classification_loss\n","#         return total_loss\n","# criterion = ImprovedGIoULoss()\n","\n","# # Optimizer\n","# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# # Training function\n","# def train_model(model, train_loader, optimizer, criterion, device):\n","#     model.train()\n","#     running_loss = 0.0\n","#     for imgs, targets in tqdm(train_loader, desc='Training', leave=False):\n","#         imgs, targets = imgs.to(device), [t.to(device) for t in targets]\n","#         optimizer.zero_grad()\n","\n","#         with torch.amp.autocast(device_type='cuda'):  # Update deprecated autcast to new format\n","#             pred_boxes, pred_conf, pred_cls = model(imgs)\n","#             # Ensure pred_boxes shape matches expected format\n","#             pred_boxes = [p.view(-1, 4) for p in pred_boxes]  # Flatten predicted boxes to [num_predictions, 4]\n","#             loss = criterion(pred_boxes, pred_conf, pred_cls, targets)\n","#         loss.backward()\n","#         optimizer.step()\n","#         running_loss += loss.item()\n","#     return running_loss / len(train_loader)\n","\n","# # Validation function\n","# def validate_model(model, val_loader, criterion, device):\n","#     model.eval()\n","#     running_loss = 0.0\n","#     with torch.no_grad():\n","#         for imgs, targets in tqdm(val_loader, desc='Validation', leave=False):\n","#             imgs, targets = imgs.to(device), [t.to(device) for t in targets]\n","\n","#             with torch.amp.autocast(device_type='cuda'):  # Update deprecated autcast to new format\n","#                 pred_boxes, pred_conf, pred_cls = model(imgs)\n","#                 # Ensure pred_boxes shape matches expected format\n","#                 pred_boxes = [p.view(-1, 4) for p in pred_boxes]  # Flatten predicted boxes to [num_predictions, 4]\n","#                 loss = criterion(pred_boxes, pred_conf, pred_cls, targets)\n","#             running_loss += loss.item()\n","#     return running_loss / len(val_loader)\n","\n","# # mAP calculation\n","# def calculate_map(model, data_loader, device, iou_threshold=0.5):\n","#     model.eval()\n","#     all_detections = []\n","#     all_annotations = []\n","#     with torch.no_grad():\n","#         for imgs, targets in tqdm(data_loader, desc='Calculating mAP', leave=False):\n","#             imgs = imgs.to(device)\n","#             pred_boxes, pred_conf, pred_cls = model(imgs)\n","\n","#             for i in range(len(imgs)):\n","#                 if i >= len(pred_boxes):\n","#                     continue\n","#                 # Collect predictions\n","#                 boxes = pred_boxes[i].view(-1, 4).cpu().numpy()\n","#                 scores = pred_conf[i].view(-1).cpu().numpy() if i < len(pred_conf) else np.array([])\n","#                 labels = pred_cls[i].view(-1).cpu().numpy() if i < len(pred_cls) else np.array([])\n","#                 all_detections.append([boxes, scores, labels])\n","\n","#                 # Collect ground-truth annotations\n","#                 all_annotations.append(targets[i].cpu().numpy())\n","\n","#     # Calculate mAP\n","#     average_precisions = []\n","#     for label in range(num_classes):\n","#         true_positives = []\n","#         scores = []\n","#         num_annotations = 0\n","\n","#         for i in range(len(all_annotations)):\n","#             detections = all_detections[i]\n","#             annotations = all_annotations[i]\n","\n","#             detections = [d for d in zip(detections[0], detections[1], detections[2]) if d[2] == label]\n","#             annotations = [a for a in annotations if a[4] == label]\n","#             num_annotations += len(annotations)\n","\n","#             detected_annotations = []\n","#             for d in detections:\n","#                 scores.append(d[1])\n","#                 if len(annotations) == 0:\n","#                     true_positives.append(0)\n","#                     continue\n","\n","#                 overlaps = box_ops.box_iou(torch.from_numpy(np.array(d[0])).unsqueeze(0), torch.from_numpy(np.array([a[:4] for a in annotations])))\n","#                 max_overlap, max_index = overlaps.max(0)\n","#                 if max_overlap >= iou_threshold and max_index not in detected_annotations:\n","#                     true_positives.append(1)\n","#                     detected_annotations.append(max_index)\n","#                 else:\n","#                     true_positives.append(0)\n","\n","#         if num_annotations == 0:\n","#             average_precisions.append(0)\n","#         else:\n","#             true_positives = np.array(true_positives)\n","#             scores = np.array(scores)\n","#             indices = np.argsort(-scores)\n","#             true_positives = true_positives[indices]\n","\n","#             false_positives = 1 - true_positives\n","#             false_positives = np.cumsum(false_positives)\n","#             true_positives = np.cumsum(true_positives)\n","\n","#             recall = true_positives / num_annotations\n","#             precision = true_positives / (true_positives + false_positives)\n","\n","#             average_precision = np.sum((recall[1:] - recall[:-1]) * precision[1:])\n","#             average_precisions.append(average_precision)\n","\n","#     return np.mean(average_precisions)\n","\n","# # Training loop\n","# best_map = 0\n","# for epoch in range(epochs):\n","#     train_loss = train_model(model, train_loader, optimizer, criterion, device)\n","#     val_loss = validate_model(model, val_loader, criterion, device)\n","#     val_map = calculate_map(model, val_loader, device)\n","\n","#     print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val mAP: {val_map:.4f}\")\n","\n","#     # Save best model based on mAP\n","\n","\n","# torch.save(model.state_dict(), 'best_yolov5_model.pth')\n","# print(f\"New best model saved at Epoch {epoch+1} with mAP {best_map:.4f}\")\n","\n","# # Test model performance on test set\n","# test_map = calculate_map(model, test_loader, device)\n","# print(f\"Test mAP: {test_map:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"Tbxqsbw3tQjQ","executionInfo":{"status":"error","timestamp":1731378548958,"user_tz":-540,"elapsed":19721,"user":{"displayName":"김형준","userId":"05139800887759179481"}},"outputId":"8214832f-0ae4-4068-9859-4d663f5c36bd"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":[]},{"output_type":"error","ename":"RuntimeError","evalue":"shape '[-1, 11]' is invalid for input of size 2899968","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-6d4cab68a784>\u001b[0m in \u001b[0;36m<cell line: 260>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0mbest_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mval_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-6d4cab68a784>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# Ensure pred_boxes shape matches expected format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mpred_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_boxes\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Flatten predicted boxes to [num_predictions, 4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-50-6d4cab68a784>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pred_boxes, pred_conf, pred_cls, targets)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpred_cls_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mpred_cls_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_cls_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure predicted classes are properly shaped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mtarget_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert target_labels to long for CrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mtarget_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpred_cls_single\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Match size if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 11]' is invalid for input of size 2899968"]}]},{"cell_type":"code","source":["\n","# # Training and evaluation\n","# def train_model(model, dataloaders, criterion, optimizer, num_epochs=epochs):\n","#     best_map = 0.0\n","\n","#     for epoch in range(num_epochs):\n","#         print(f'Epoch {epoch + 1}/{num_epochs}')\n","#         print('-' * 30)\n","\n","#         # Each epoch has a training and validation phase\n","#         for phase in ['train', 'valid']:\n","#             if phase == 'train':\n","#                 model.train()  # Set model to training mode\n","#             else:\n","#                 model.eval()  # Set model to evaluate mode\n","\n","#             running_loss = 0.0\n","#             all_predictions = []\n","#             all_targets = []\n","\n","#             # Iterate over data\n","#             for inputs, labels in tqdm(dataloaders[phase]):\n","#                 inputs, labels = inputs.to(device), labels.to(device)\n","#                 optimizer.zero_grad()\n","\n","#                 with torch.set_grad_enabled(phase == 'train'):\n","#                     # Forward pass\n","#                     pred_boxes, pred_conf, pred_cls = model(inputs)\n","\n","#                     # Compute loss\n","#                     loss = criterion(pred_cls[0], labels)  # Assuming pred_cls[0] corresponds to small scale\n","\n","#                     if phase == 'train':\n","#                         loss.backward()\n","#                         optimizer.step()\n","\n","#                 # Statistics\n","#                 running_loss += loss.item() * inputs.size(0)\n","#                 all_predictions.append(pred_cls)  # Store predictions\n","#                 all_targets.append(labels)  # Store targets\n","\n","#             epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","\n","#             # Calculate mAP for validation phase\n","#             if phase == 'valid':\n","#                 mAP = compute_map(all_predictions, all_targets)\n","#                 print(f'Validation mAP: {mAP:.4f}')\n","#                 if mAP > best_map:\n","#                     best_map = mAP\n","#                     torch.save(model.state_dict(), 'best_model.pth')\n","\n","#             print(f'{phase} Loss: {epoch_loss:.4f}')\n","\n","#     print(f'Best Validation mAP: {best_map:.4f}')\n","\n","# # Train the model\n","# train_model(model, dataloaders, criterion, optimizer)\n","\n","# # Load the best model for evaluation\n","# model.load_state_dict(torch.load('best_model.pth'))\n","# model.eval()\n","\n","# # Test set evaluation\n","# def evaluate_model(model, dataloader):\n","#     model.eval()\n","#     all_predictions = []\n","#     all_targets = []\n","\n","#     with torch.no_grad():\n","#         for inputs, labels in tqdm(dataloader):\n","#             inputs, labels = inputs.to(device), labels.to(device)\n","#             pred_boxes, pred_conf, pred_cls = model(inputs)\n","#             all_predictions.append(pred_cls)  # Store predictions\n","#             all_targets.append(labels)  # Store targets\n","\n","#     # Compute mAP\n","#     mAP = compute_map(all_predictions, all_targets)\n","#     print(f'Test mAP: {mAP:.4f}')\n","\n","# # Evaluate the model on the test set\n","# evaluate_model(model, dataloaders['test'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":465},"id":"GlUJZkQqTgM3","executionInfo":{"status":"error","timestamp":1731330087416,"user_tz":-540,"elapsed":380,"user":{"displayName":"김형준","userId":"05139800887759179481"}},"outputId":"ba6c8114-8409-4758-c814-d13ecfee65f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/1 [00:00<?, ?it/s]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"only batches of spatial targets supported (3D tensors) but got targets of size: : [16, 5]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-72e639251ef0>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Load the best model for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-72e639251ef0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_cls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming pred_cls[0] corresponds to small scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of size: : [16, 5]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wVBPe6W_F0qW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TbopORZnF0uG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Dx2pxwG-MkMC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ox0QNq7qTaJA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J4POGLWIF0xc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"b0-QaLVQn7Yu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VlGoqyp2T-kX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vkTH1WWXa9O9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bpA-v8pjm3_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"034tIoDLM6Xy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_ZC_pDrgUTMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"HkM__X7EUaN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FyEvUYw_UcTD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hvZMV7WHM3zI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TS4i9yluM82s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TAUJR-YmF01V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kzN3DwczKDOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Nx2Or2I5F05J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PZyrXYvWF09B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qAob9kbmF1A0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"2yFT90rrF1EH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7qgolbAuF1Hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4dIAq1SZF1K8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"zJ22dl8-F1OM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"DsU7CkNCF1Rj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rI1pQOlzF1U1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hnsCgH65F1Xy"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4","authorship_tag":"ABX9TyO3AFEYDYwADrxDmN/J7rw5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}